import os
import json
import re
from typing import Optional, Dict, Any, Literal
from pathlib import Path
from groq import Groq


class AgentModeles:
    """
    Agent sp√©cialis√© dans la g√©n√©ration automatique de fonctions Python
    bas√©es sur des mod√®les IA via l'API Groq.
    
    Supporte plusieurs types de mod√®les:
    - LLM (text generation)
    - Speech-to-Text (transcription)
    - Text-to-Speech (synthesis)
    - Text-to-Video (generation)
    - Image generation
    """
    
    # Configuration des mod√®les par type
    MODEL_CONFIG = {
        "llm": {
            "model": "openai/gpt-oss-120b",
            "description": "Large Language Model pour g√©n√©ration de texte"
        },
        "speech_to_text": {
            "model": "whisper-large-v3",
            "description": "Mod√®le de transcription audio vers texte"
        },
        "text_to_speech": {
            "model": "playai-tts",
            "description": "Mod√®le de synth√®se vocale texte vers audio"
        },
        "text_to_video": {
            "model": "stable-video-diffusion",
            "description": "Mod√®le de g√©n√©ration de vid√©o depuis texte"
        },
        "image_generation": {
            "model": "dall-e-3",
            "description": "Mod√®le de g√©n√©ration d'images depuis texte"
        }
    }
    
    PROMPTS_DIR = "prompts"
    
    def __init__(self):
        """
        Initialise l'agent avec le client Groq.
        R√©cup√®re la cl√© API depuis les variables d'environnement.
        """
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            raise ValueError(
                "GROQ_API_KEY non trouv√©e. "
                "Veuillez d√©finir la variable d'environnement GROQ_API_KEY."
            )
        self.client = Groq(api_key=api_key)
        self._ensure_prompts_dir()
    
    def _ensure_prompts_dir(self) -> None:
        """Cr√©e le r√©pertoire des prompts s'il n'existe pas."""
        Path(self.PROMPTS_DIR).mkdir(exist_ok=True)
    
    def _load_prompt_template(self, filename: str) -> str:
        """
        Charge un template de prompt depuis un fichier txt.
        
        Args:
            filename: Nom du fichier dans le r√©pertoire prompts/
        
        Returns:
            Contenu du fichier
        """
        script_dir = Path(__file__).parent
        prompts_path = script_dir / self.PROMPTS_DIR
        filepath = prompts_path / filename
        
        try:
            return filepath.read_text(encoding='utf-8')
        except FileNotFoundError:
            raise FileNotFoundError(
                f"Fichier prompt non trouv√©: {filepath}"
            )
    
    def generate_model_function(
        self,
        description: str,
        inputs: Dict[str, str],
        outputs: Dict[str, str],
        model_type: Literal["llm", "speech_to_text", "text_to_speech", 
                           "text_to_video", "image_generation"] = "llm",
        constraints: Optional[str] = None,
        temperature: float = 0.3,
        max_tokens: int = 2048
    ) -> Dict[str, Any]:
        """
        G√©n√®re une fonction Python IA bas√©e sur une description.
        
        Args:
            description: Description textuelle de la fonction √† g√©n√©rer
            inputs: Dictionnaire {nom_param: type_param}
            outputs: Dictionnaire {nom_return: type_return}
            model_type: Type de mod√®le √† utiliser
            constraints: Contraintes additionnelles (optionnel)
            temperature: Temp√©rature pour la g√©n√©ration (0.0-1.0)
            max_tokens: Nombre maximum de tokens √† g√©n√©rer
        
        Returns:
            Dict structur√© contenant le code g√©n√©r√© et m√©tadonn√©es d√©taill√©es
        """
        # Validation du type de mod√®le
        if model_type not in self.MODEL_CONFIG:
            raise ValueError(
                f"Type de mod√®le invalide: {model_type}. "
                f"Types disponibles: {list(self.MODEL_CONFIG.keys())}"
            )
        
        # R√©cup√©ration de la configuration du mod√®le
        model_info = self.MODEL_CONFIG[model_type]
        
        # Construction du prompt
        prompt = self.build_prompt(
            description=description,
            inputs=inputs,
            outputs=outputs,
            model_type=model_type,
            model_name=model_info["model"],
            constraints=constraints
        )
        
        # Appel au LLM
        llm_output = self.call_llm(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # Parsing de la sortie
        parsed = self.parse_llm_output(llm_output)
        
        # Construction du contexte professionnel d√©taill√©
        context = self._build_detailed_context(
            description=description,
            inputs=inputs,
            outputs=outputs,
            model_type=model_type,
            model_info=model_info,
            constraints=constraints,
            parsed=parsed
        )
        
        return {
            "source_code": parsed["code"],
            "context": context,
            "prompt": prompt,
            "metadata": {
                "fonction": {
                    "nom": parsed["function_name"],
                    "input": inputs,
                    "output": outputs,
                    "descriptif": description
                },
                "modele": {
                    "type": model_type,
                    "nom": model_info["model"],
                    "description": model_info["description"]
                },
                "generation": {
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }
            }
        }
    
    def _build_detailed_context(
        self,
        description: str,
        inputs: Dict[str, str],
        outputs: Dict[str, str],
        model_type: str,
        model_info: Dict[str, str],
        constraints: Optional[str],
        parsed: Dict[str, Any]
    ) -> str:
        """
        Construit un contexte d√©taill√© et professionnel pour la fonction g√©n√©r√©e.
        
        Args:
            description: Description de la fonction
            inputs: Param√®tres d'entr√©e
            outputs: Param√®tres de sortie
            model_type: Type de mod√®le utilis√©
            model_info: Informations sur le mod√®le
            constraints: Contraintes appliqu√©es
            parsed: R√©sultat du parsing
        
        Returns:
            Contexte format√© en markdown professionnel
        """
        context_parts = [
            "# Fonction G√©n√©r√©e par AgentModeles",
            "",
            "## üìã Informations G√©n√©rales",
            "",
            f"**Nom de la fonction:** `{parsed['function_name']}`",
            f"**Description:** {description}",
            f"**Type de mod√®le:** {model_type}",
            f"**Mod√®le IA utilis√©:** {model_info['model']}",
            f"**Description du mod√®le:** {model_info['description']}",
            "",
            "## üì• Param√®tres d'Entr√©e",
            ""
        ]
        
        for param_name, param_type in inputs.items():
            context_parts.append(f"- **{param_name}** (`{param_type}`)")
        
        context_parts.extend([
            "",
            "## üì§ Param√®tres de Sortie",
            ""
        ])
        
        for output_name, output_type in outputs.items():
            context_parts.append(f"- **{output_name}** (`{output_type}`)")
        
        if constraints:
            context_parts.extend([
                "",
                "## ‚ö†Ô∏è Contraintes Appliqu√©es",
                "",
                constraints
            ])
        
        context_parts.extend([
            "",
            "## üîß D√©tails Techniques",
            "",
            "### Configuration API",
            "- **Provider:** Groq API",
            "- **Authentification:** Variable d'environnement `GROQ_API_KEY`",
            f"- **Endpoint du mod√®le:** `{model_info['model']}`",
            "",
            "### Sp√©cificit√©s du Type de Mod√®le",
            ""
        ])
        
        # Ajout de d√©tails sp√©cifiques selon le type de mod√®le
        if model_type == "speech_to_text":
            context_parts.extend([
                "- **Type:** Transcription audio vers texte",
                "- **Formats support√©s:** MP3, WAV, M4A, FLAC",
                "- **Langues:** Multi-langues avec d√©tection automatique",
                "- **Qualit√©:** Haute pr√©cision avec timestamps disponibles"
            ])
        elif model_type == "text_to_speech":
            context_parts.extend([
                "- **Type:** Synth√®se vocale",
                "- **Format de sortie:** Audio encod√© (MP3/WAV)",
                "- **Voix:** Configurable selon disponibilit√©",
                "- **Qualit√©:** Haute fid√©lit√© naturelle"
            ])
        elif model_type == "text_to_video":
            context_parts.extend([
                "- **Type:** G√©n√©ration vid√©o depuis description",
                "- **Format de sortie:** Vid√©o MP4",
                "- **R√©solution:** Configurable",
                "- **Dur√©e:** Variable selon configuration"
            ])
        elif model_type == "image_generation":
            context_parts.extend([
                "- **Type:** G√©n√©ration d'images depuis texte",
                "- **Format de sortie:** PNG/JPEG",
                "- **R√©solution:** Haute qualit√© configurable",
                "- **Style:** Personnalisable via prompt"
            ])
        else:  # llm
            context_parts.extend([
                "- **Type:** Large Language Model",
                "- **Capacit√©s:** G√©n√©ration de texte, analyse, raisonnement",
                "- **Contexte:** Jusqu'√† 8K tokens",
                "- **Temperature:** Ajustable pour contr√¥ler la cr√©ativit√©"
            ])
        
        context_parts.extend([
            "",
            "## üìù Utilisation",
            "",
            "```python",
            f"# Exemple d'utilisation de {parsed['function_name']}",
            "from groq import Groq",
            "import os",
            "",
            "# La fonction utilise automatiquement GROQ_API_KEY",
            f"result = {parsed['function_name']}(",
        ])
        
        # Ajouter les param√®tres d'exemple
        for i, param_name in enumerate(inputs.keys()):
            comma = "," if i < len(inputs) - 1 else ""
            context_parts.append(f"    {param_name}=<valeur>{comma}")
        
        context_parts.extend([
            ")",
            "",
            "# Acc√©der aux r√©sultats",
        ])
        
        for output_name in outputs.keys():
            context_parts.append(f"print(result['{output_name}'])")
        
        context_parts.extend([
            "```",
            "",
            "## üõ°Ô∏è Gestion des Erreurs",
            "",
            "La fonction g√®re automatiquement:",
            "- Validation des param√®tres d'entr√©e",
            "- V√©rification de la pr√©sence de `GROQ_API_KEY`",
            "- Gestion des erreurs API Groq",
            "- Retour structur√© avec toutes les cl√©s requises",
            "",
            "## üìö D√©pendances",
            "",
            "```bash",
            "pip install groq",
            "```",
            "",
            "## üîê Configuration Requise",
            "",
            "```bash",
            "export GROQ_API_KEY='votre-cl√©-api'",
            "# ou dans .env",
            "GROQ_API_KEY=votre-cl√©-api",
            "```",
            "",
            "---",
            "",
            f"*G√©n√©r√© automatiquement par AgentModeles v2.0 - {model_type.replace('_', ' ').title()}*"
        ])
        
        return "\n".join(context_parts)
    
    def build_prompt(
        self,
        description: str,
        inputs: Dict[str, str],
        outputs: Dict[str, str],
        model_type: str,
        model_name: str,
        constraints: Optional[str] = None
    ) -> str:
        """
        Construit le prompt √† envoyer au mod√®le LLM.
        Charge les templates depuis des fichiers txt.
        
        Args:
            description: Description de la fonction
            inputs: Dictionnaire des param√®tres d'entr√©e
            outputs: Dictionnaire des param√®tres de sortie
            model_type: Type de mod√®le (llm, speech_to_text, etc.)
            model_name: Nom exact du mod√®le Groq
            constraints: Contraintes additionnelles
        
        Returns:
            Prompt format√© pour le LLM
        """
        # Charger le template appropri√© selon le type de mod√®le
        template_file = f"prompt_{model_type}.txt"
        
        try:
            template = self._load_prompt_template(template_file)
        except FileNotFoundError:
            # Fallback sur le template par d√©faut
            template = self._load_prompt_template("prompt_main.txt")
        
        # Charger les sections communes
        inputs_section = self._load_prompt_template("inputs_section.txt")
        outputs_section = self._load_prompt_template("outputs_section.txt")
        instructions_section = self._load_prompt_template("instructions_section.txt")
        format_section = self._load_prompt_template(f"format_{model_type}.txt")
        
        # Formater les inputs
        inputs_str = "\n".join(
            [f"  - {name}: {type_}" for name, type_ in inputs.items()]
        )
        inputs_formatted = inputs_section.format(inputs=inputs_str)
        
        # Formater les outputs
        outputs_str = "\n".join(
            [f"  - {name}: {type_}" for name, type_ in outputs.items()]
        )
        outputs_formatted = outputs_section.format(outputs=outputs_str)
        
        # Formater les contraintes
        constraints_formatted = ""
        if constraints:
            constraints_section_template = self._load_prompt_template(
                "constraints_section.txt"
            )
            constraints_formatted = constraints_section_template.format(
                constraints=constraints
            )
        
        # Assembler le prompt complet
        prompt = template.format(
            description=description,
            model_name=model_name,
            inputs_section=inputs_formatted,
            outputs_section=outputs_formatted,
            constraints_section=constraints_formatted,
            instructions_section=instructions_section,
            format_section=format_section
        )
        
        return prompt
    
    def _build_system_context(self, model_type: str) -> str:
        """
        Construit un contexte syst√®me d√©taill√© pour le LLM selon le type de mod√®le.
        
        Args:
            model_type: Type de mod√®le cible
        
        Returns:
            Contexte syst√®me riche et professionnel
        """
        base_context = """Tu es un Architecte Logiciel Senior sp√©cialis√© dans l'ing√©nierie IA et l'int√©gration d'APIs de mod√®les g√©n√©ratifs. 

Tu poss√®des une expertise approfondie dans:

üéØ COMP√âTENCES PRINCIPALES:
- Architecture de syst√®mes d'IA en production avec plus de 10 ans d'exp√©rience
- D√©veloppement Python avanc√© (PEP 8, type hints, design patterns)
- Int√©gration d'APIs Groq et mod√®les de Machine Learning
- Conception de fonctions robustes, testables et maintenables
- Gestion d'erreurs exhaustive et validation de donn√©es stricte
- Documentation technique de niveau entreprise
- S√©curit√© applicative et bonnes pratiques DevSecOps
- Optimisation de performance et gestion de ressources

üíº TON R√îLE:
G√©n√©rer du code Python de qualit√© PRODUCTION (pas de prototype!) qui:
1. Fonctionne imm√©diatement sans modification
2. G√®re tous les cas limites et erreurs possibles
3. Suit les standards industriels (Clean Code, SOLID)
4. Inclut une documentation compl√®te et professionnelle
5. Est s√©curis√© contre les vuln√©rabilit√©s courantes
6. Peut √™tre d√©ploy√© en environnement critique

‚ö° PHILOSOPHIE DE CODE:
- "Fail fast, fail explicitly" - d√©tection rapide des erreurs
- "No surprises" - comportement pr√©visible et document√©
- "Production-first" - code pr√™t pour la production d√®s la g√©n√©ration
- "Type-safe" - utilisation maximale des type hints Python
- "Self-documenting" - code lisible qui s'explique lui-m√™me

üîí EXIGENCES DE S√âCURIT√â:
- Validation stricte de TOUTES les entr√©es utilisateur
- Pas d'injection de code (eval, exec, subprocess)
- Gestion s√©curis√©e des secrets (variables d'environnement uniquement)
- Traitement appropri√© des donn√©es sensibles
- Logging des erreurs sans exposer d'informations sensibles

"""
        
        # Contexte sp√©cifique selon le type de mod√®le
        specific_contexts = {
            "llm": """
üìö SP√âCIALISATION LLM:
Tu es expert en:
- Prompt engineering et optimisation de prompts
- Gestion de contexte et fen√™tres de tokens
- Streaming de r√©ponses pour UX am√©lior√©e
- Cha√Ænage de prompts et workflows complexes
- Extraction structur√©e depuis texte non-structur√©
- Gestion de la temp√©rature et sampling pour qualit√© optimale

Tu g√©n√®res des fonctions qui utilisent les LLMs pour:
- Analyse de texte et extraction d'informations
- G√©n√©ration de contenu cr√©atif et professionnel
- Traduction et reformulation intelligente
- R√©sum√© et synth√®se de documents
- Classification et cat√©gorisation
- Dialogue conversationnel contextuel
""",
            "speech_to_text": """
üé§ SP√âCIALISATION SPEECH-TO-TEXT:
Tu es expert en:
- Traitement audio et formats multimedia (MP3, WAV, FLAC, M4A)
- Mod√®les Whisper et leurs capacit√©s multilingues
- Gestion de fichiers volumineux et chunking audio
- Extraction de m√©tadonn√©es (timestamps, speakers, langue)
- Optimisation de la qualit√© de transcription
- Gestion de contexte audio (bruit, accents, domaines techniques)

Tu g√©n√®res des fonctions qui:
- Transcrivent avec pr√©cision dans 99+ langues
- D√©tectent automatiquement la langue source
- Fournissent timestamps au niveau mot/phrase/paragraphe
- G√®rent l'identification de locuteurs (diarization)
- Supportent les fichiers audio jusqu'√† 25MB
- Retournent des m√©tadonn√©es riches (dur√©e, confiance, langue)
""",
            "text_to_speech": """
üîä SP√âCIALISATION TEXT-TO-SPEECH:
Tu es expert en:
- Synth√®se vocale naturelle et expressive
- Mod√®les TTS avanc√©s (PlayAI, ElevenLabs patterns)
- Contr√¥le prosodique (vitesse, ton, √©motion)
- Gestion de voix multilingues et multi-speakers
- Encodage audio optimal (MP3, Opus, AAC)
- Streaming audio pour latence minimale

Tu g√©n√®res des fonctions qui:
- Produisent une voix naturelle et engageante
- Supportent plusieurs voix et styles (casual/professional/narrative)
- Contr√¥lent vitesse (0.25x-4.0x) et expressivit√©
- G√®rent les longues transcriptions avec chunking intelligent
- Retournent audio en base64 ou fichier direct
- Optimisent la qualit√© audio (bitrate, sample rate)
""",
            "text_to_video": """
üé¨ SP√âCIALISATION TEXT-TO-VIDEO:
Tu es expert en:
- G√©n√©ration vid√©o depuis descriptions textuelles
- Mod√®les de diffusion vid√©o (Stable Video, Runway patterns)
- Contr√¥le de style, r√©solution et dur√©e
- Gestion de prompts visuels complexes
- Optimisation de rendu et qualit√© d'image
- Formats vid√©o et encodage (MP4, WebM)

Tu g√©n√®res des fonctions qui:
- Cr√©ent des vid√©os HD/4K depuis descriptions d√©taill√©es
- Contr√¥lent dur√©e (3-30s), FPS et r√©solution
- Supportent diff√©rents styles (r√©aliste, artistique, cin√©matique)
- G√®rent les prompts n√©gatifs pour √©viter contenu ind√©sirable
- Retournent URL ou fichier vid√©o avec thumbnail
- Incluent m√©tadonn√©es compl√®tes (codec, bitrate, dimensions)
""",
            "image_generation": """
üé® SP√âCIALISATION IMAGE GENERATION:
Tu es expert en:
- G√©n√©ration d'images par IA (DALL-E, Stable Diffusion patterns)
- Prompt engineering visuel avanc√©
- Contr√¥le de style, composition et qualit√©
- R√©solutions multiples et ratios d'aspect
- Post-processing et am√©lioration d'images
- Formats optimaux (PNG, JPEG, WebP)

Tu g√©n√®res des fonctions qui:
- Cr√©ent des images haute qualit√© depuis descriptions
- Supportent multiples tailles (256px √† 1792px)
- Contr√¥lent style (vivid/natural/artistic)
- Optimisent les prompts automatiquement (revised prompt)
- Retournent images en base64 ou URL
- G√®rent qualit√© standard/HD selon besoin
- Incluent m√©tadonn√©es (dimensions, format, prompt optimis√©)
"""
        }
        
        specific = specific_contexts.get(model_type, specific_contexts["llm"])
        
        final_context = base_context + specific + """

üéì STANDARDS DE CODE √Ä RESPECTER:
1. Type hints sur TOUTES les fonctions et param√®tres
2. Docstrings Google-style avec Args, Returns, Raises
3. Validation d'entr√©e exhaustive avec messages d'erreur explicites
4. Try-except cibl√©s avec gestion d'erreur appropri√©e
5. Nommage descriptif (variables, fonctions explicites)
6. Retour structur√© en dictionnaire avec cl√©s document√©es
7. Constantes en MAJUSCULES, pas de magic numbers
8. Logging appropri√© sans exposer de secrets
9. Code DRY (Don't Repeat Yourself)
10. Complexit√© cyclomatique < 10 par fonction

‚úÖ CHECKLIST AVANT G√âN√âRATION:
- [ ] Imports minimaux et n√©cessaires uniquement
- [ ] Validation de tous les param√®tres d'entr√©e
- [ ] Gestion GROQ_API_KEY via os.getenv()
- [ ] Client Groq initialis√© proprement
- [ ] Appel API avec param√®tres appropri√©s
- [ ] Extraction et transformation des r√©sultats
- [ ] Dictionnaire de retour avec TOUTES les cl√©s requises
- [ ] Gestion d'erreurs compl√®te (ValueError, FileNotFoundError, APIError, etc.)
- [ ] Docstring compl√®te et exemples d'utilisation
- [ ] Code pr√™t pour tests unitaires

üöÄ G√âN√àRE MAINTENANT DU CODE DE NIVEAU SENIOR ENGINEER!
"""
        
        return final_context.strip()
    
    def call_llm(
        self,
        prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 2048
    ) -> str:
        """
        Appelle le mod√®le Groq pour g√©n√©ration de code.
        
        Args:
            prompt: Prompt √† envoyer au mod√®le
            temperature: Contr√¥le la cr√©ativit√© (0.0-1.0)
            max_tokens: Nombre maximum de tokens √† g√©n√©rer
        
        Returns:
            R√©ponse brute du mod√®le
        """
        # Extraire le type de mod√®le du prompt si possible
        model_type = "llm"  # default
        for mtype in self.MODEL_CONFIG.keys():
            if mtype in prompt.lower():
                model_type = mtype
                break
        
        try:
            # Construire un contexte syst√®me d√©taill√© et professionnel
            system_context = self._build_system_context(model_type)
            
            message = self.client.chat.completions.create(
                model=self.MODEL_CONFIG["llm"]["model"],
                messages=[
                    {
                        "role": "system",
                        "content": system_context
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=0.9
            )
            return message.choices[0].message.content
        except Exception as e:
            raise RuntimeError(
                f"Erreur lors de l'appel √† l'API Groq: {str(e)}"
            ) from e
    
    def parse_llm_output(self, llm_raw_output: str) -> Dict[str, Any]:
        """
        Parse la r√©ponse brute du LLM pour extraire le code.
        
        Args:
            llm_raw_output: R√©ponse brute du mod√®le
        
        Returns:
            Dict avec 'code', 'function_name' et 'context'
        """
        code = ""
        function_name = "generated_function"
        context = ""
        
        # Extraction du code Python entre ```python et ```
        python_match = re.search(
            r"```python\s*(.*?)\s*```",
            llm_raw_output,
            re.DOTALL
        )
        
        if python_match:
            code = python_match.group(1).strip()
        else:
            # Fallback: chercher juste entre ``` et ```
            code_match = re.search(
                r"```\s*(.*?)\s*```",
                llm_raw_output,
                re.DOTALL
            )
            if code_match:
                code = code_match.group(1).strip()
            else:
                # Si pas de markdown, prendre la sortie enti√®re
                code = llm_raw_output.strip()
        
        # Extraction du nom de fonction
        def_match = re.search(r"def\s+(\w+)\s*\(", code)
        if def_match:
            function_name = def_match.group(1)
        
        # Contexte = ce qui n'est pas du code
        context = llm_raw_output.replace(f"```python\n{code}\n```", "").strip()
        if not context:
            context = "Fonction g√©n√©r√©e par AgentModeles via Groq"
        
        return {
            "code": code,
            "function_name": function_name,
            "context": context
        }
    
    def get_available_models(self) -> Dict[str, Dict[str, str]]:
        """
        Retourne la liste des mod√®les disponibles et leurs descriptions.
        
        Returns:
            Dictionnaire des mod√®les disponibles
        """
        return self.MODEL_CONFIG.copy()